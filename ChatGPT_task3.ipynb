{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import openai \n",
    "import ast\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>ID</th>\n",
       "      <th>Impressions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U13000</td>\n",
       "      <td>N42782 N18445 N49749</td>\n",
       "      <td>N7482-1 N6379-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U14000</td>\n",
       "      <td>N9721 N61358 N11360 N64593 N9900 N46688</td>\n",
       "      <td>N1145-1 N34930-1 N53526-0 N3574-0 N751-0 N2202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U15000</td>\n",
       "      <td>N42474 N41013 N45970 N4015 N56461 N11161 N31633</td>\n",
       "      <td>N4303-1 N58730-0 N64130-0 N38064-0 N49210-0 N92-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U13740</td>\n",
       "      <td>N55189 N42782 N34694 N45794 N18445 N63302 N104...</td>\n",
       "      <td>N55689-1 N35729-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U91836</td>\n",
       "      <td>N31739 N6072 N63045 N23979 N35656 N43353 N8129...</td>\n",
       "      <td>N20678-0 N39317-0 N58114-0 N20495-0 N42977-0 N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     User                                                 ID  \\\n",
       "0  U13000                               N42782 N18445 N49749   \n",
       "1  U14000            N9721 N61358 N11360 N64593 N9900 N46688   \n",
       "2  U15000    N42474 N41013 N45970 N4015 N56461 N11161 N31633   \n",
       "3  U13740  N55189 N42782 N34694 N45794 N18445 N63302 N104...   \n",
       "4  U91836  N31739 N6072 N63045 N23979 N35656 N43353 N8129...   \n",
       "\n",
       "                                         Impressions  \n",
       "0                                    N7482-1 N6379-0  \n",
       "1  N1145-1 N34930-1 N53526-0 N3574-0 N751-0 N2202...  \n",
       "2  N4303-1 N58730-0 N64130-0 N38064-0 N49210-0 N92-0  \n",
       "3                                  N55689-1 N35729-0  \n",
       "4  N20678-0 N39317-0 N58114-0 N20495-0 N42977-0 N...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors = pd.read_csv(\"MIND/behaviors_testdataset.csv\", sep=';', header=None)\n",
    "behaviors.columns =['User', 'Time', 'ID', 'Impressions'] \n",
    "behaviors = behaviors.drop(['Time'], axis=1)\n",
    "behaviors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>SubCategory</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N53526</td>\n",
       "      <td>health</td>\n",
       "      <td>voices</td>\n",
       "      <td>I Was An NBA Wife. Here's How It Affected My M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N9721</td>\n",
       "      <td>health</td>\n",
       "      <td>nutrition</td>\n",
       "      <td>50 Foods You Should Never Eat, According to He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N39758</td>\n",
       "      <td>health</td>\n",
       "      <td>nutrition</td>\n",
       "      <td>25 Biggest Grocery Store Mistakes Making You G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N3574</td>\n",
       "      <td>autos</td>\n",
       "      <td>autosnews</td>\n",
       "      <td>Ford Bronco Test Mule Spotted Flexing Its Musc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N42474</td>\n",
       "      <td>news</td>\n",
       "      <td>newsbusiness</td>\n",
       "      <td>Trump's Trustbusters Bring Microsoft Lessons t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID Category   SubCategory  \\\n",
       "0  N53526   health        voices   \n",
       "1   N9721   health     nutrition   \n",
       "2  N39758   health     nutrition   \n",
       "3   N3574    autos     autosnews   \n",
       "4  N42474     news  newsbusiness   \n",
       "\n",
       "                                             Content  \n",
       "0  I Was An NBA Wife. Here's How It Affected My M...  \n",
       "1  50 Foods You Should Never Eat, According to He...  \n",
       "2  25 Biggest Grocery Store Mistakes Making You G...  \n",
       "3  Ford Bronco Test Mule Spotted Flexing Its Musc...  \n",
       "4  Trump's Trustbusters Bring Microsoft Lessons t...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = pd.read_csv(\"MIND/news_testdataset.csv\") \n",
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_related_content(user):\n",
    "    user_row = behaviors[behaviors['User'] == user]\n",
    "    \n",
    "    if user_row.empty:\n",
    "        return [(\"No data found for user\", user)]\n",
    "    \n",
    "    user_ids = user_row['ID'].iloc[0].split()\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for user_id in user_ids:\n",
    "        id_row = news[news['ID'] == user_id]\n",
    "        \n",
    "        if not id_row.empty:\n",
    "            content = id_row['Content'].iloc[0]\n",
    "            result.append((user_id, content))\n",
    "    \n",
    "    user_output = [f'{item[0]}: {item[1]}' for item in result]\n",
    "    \n",
    "    return user_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_id(id_value):\n",
    "    # Remove index part (\"-1\" or \"-0\")\n",
    "    return id_value.rstrip('01').rstrip('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_related_impressions(user):\n",
    "    user_row = behaviors[behaviors['User'] == user]\n",
    "    \n",
    "    if user_row.empty:\n",
    "        return [(\"No data found for user\", user)]\n",
    "    \n",
    "    user_ids = user_row['Impressions'].iloc[0].split()\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for user_id in user_ids:\n",
    "        cleaned_id = clean_id(user_id)\n",
    "        id_row = news[news['ID'] == cleaned_id]\n",
    "        \n",
    "        if not id_row.empty:\n",
    "            content = id_row['Content'].iloc[0]\n",
    "            result.append((cleaned_id, content))\n",
    "            \n",
    "    impressions_output = [f'{item[0]}: {item[1]}' for item in result]\n",
    "    \n",
    "    return impressions_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to chat with GPT\n",
    "def chat_with_chatgpt(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "    message = response['choices'][0]['message']['content'].strip()\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_task3 (user):\n",
    "    user_output = get_user_related_content(user)\n",
    "    impressions_output = get_user_related_impressions(user)\n",
    "    \n",
    "    user_prompt = (f\"The user has interacted with the items from List 1: {user_output}. From the List 2 choose some items, which you could recommend for this user. List 2: {impressions_output}. Output format: a python list with news index (e.g., N12345). Do not explain the reason or include any other words.\")\n",
    "    \n",
    "    chatbot_response = chat_with_chatgpt(user_prompt)\n",
    "    \n",
    "    # Remove leading and trailing whitespaces and newline characters\n",
    "    cleaned_string = chatbot_response.strip()\n",
    "\n",
    "    # Use ast.literal_eval to safely evaluate the string as a literal expression\n",
    "    result_list = ast.literal_eval(cleaned_string)\n",
    "    \n",
    "    return (result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evaluation_df (user):\n",
    "    user_row = behaviors[behaviors['User'] == user]\n",
    "    user_ids = user_row['Impressions'].iloc[0].split()\n",
    "    list_1 = user_ids\n",
    "    list_2 = None\n",
    "    try:\n",
    "        list_2 = gpt_task3(user)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing user {user}: {e}\")\n",
    "        \n",
    "    if list_2 is not None:\n",
    "        # Your code that iterates over list_2 or uses it goes here\n",
    "        for item in list_2:\n",
    "            # Process each item in list_2\n",
    "            pass\n",
    "    \n",
    "    #user_row = behaviors[behaviors['User'] == user]\n",
    "    #user_ids = user_row['Impressions'].iloc[0].split()\n",
    "    # Example lists\n",
    "    #list_1 = user_ids\n",
    "    #list_2 = gpt_task3(user)\n",
    "    \n",
    "    #create a DataFrame with the 'ID' column from list_1\n",
    "    final_df = pd.DataFrame({'ID': [item.split('-')[0] for item in list_1]})\n",
    "\n",
    "    # Add the 'True' column based on the indexes (1 or 0)\n",
    "    final_df['True'] = [int(item.split('-')[1]) for item in list_1]\n",
    "\n",
    "    # Add the 'Predicted' column based on the presence in list_2\n",
    "    final_df['Predicted'] = [1 if item in list_2 else 0 for item in final_df['ID']]\n",
    "\n",
    "    # Display the final DataFrame\n",
    "    #print(list_1)\n",
    "    #print(list_2)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall (user):\n",
    "    df = create_evaluation_df(user)\n",
    "    # Calculate precision\n",
    "    precision = precision_score(df['True'], df['Predicted'])\n",
    "\n",
    "    # Calculate recall\n",
    "    recall = recall_score(df['True'], df['Predicted'])\n",
    "\n",
    "    # Calculate weighted precision\n",
    "    weighted_precision = precision_score(df['True'], df['Predicted'], average='weighted')\n",
    "\n",
    "    # Calculate weighted recall\n",
    "    weighted_recall = recall_score(df['True'], df['Predicted'], average='weighted')\n",
    "\n",
    "    # Print the results\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'Weighted Precision: {weighted_precision}')\n",
    "    print(f'Weighted Recall: {weighted_recall}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N55689-1', 'N35729-0']\n",
      "['N55689', 'N35729']\n",
      "Precision: 0.5\n",
      "Recall: 1.0\n",
      "Weighted Precision: 0.25\n",
      "Weighted Recall: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "user_id = \"U13740\"\n",
    "calculate_precision_recall(user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['U13000', 'U14000', 'U15000', 'U13740', 'U91836', 'U73700', 'U34670', 'U8125', 'U19739', 'U8355', 'U46596', 'U79199', 'U53231', 'U89744', 'U10045', 'U92486', 'U29155', 'U63162', 'U17841', 'U8312', 'U11306', 'U38627', 'U1111']\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame named 'users' with a 'User' column\n",
    "users_list = behaviors['User'].unique().tolist()\n",
    "\n",
    "# Print the resulting list\n",
    "print(users_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing user U13740: malformed node or string on line 1: <ast.Name object at 0x164c7dcc0>\n",
      "Error processing user U13740: argument of type 'NoneType' is not iterable\n",
      "Error processing user U91836: This model's maximum context length is 4097 tokens. However, your messages resulted in 5003 tokens. Please reduce the length of the messages.\n",
      "Error processing user U91836: argument of type 'NoneType' is not iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing user U8355: This model's maximum context length is 4097 tokens. However, your messages resulted in 9980 tokens. Please reduce the length of the messages.\n",
      "Error processing user U8355: argument of type 'NoneType' is not iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing user U89744: This model's maximum context length is 4097 tokens. However, your messages resulted in 16978 tokens. Please reduce the length of the messages.\n",
      "Error processing user U89744: argument of type 'NoneType' is not iterable\n",
      "Error processing user U10045: malformed node or string on line 1: <ast.Name object at 0x164c7cd00>\n",
      "Error processing user U10045: argument of type 'NoneType' is not iterable\n",
      "Error processing user U92486: This model's maximum context length is 4097 tokens. However, your messages resulted in 7158 tokens. Please reduce the length of the messages.\n",
      "Error processing user U92486: argument of type 'NoneType' is not iterable\n",
      "Error processing user U29155: This model's maximum context length is 4097 tokens. However, your messages resulted in 4194 tokens. Please reduce the length of the messages.\n",
      "Error processing user U29155: argument of type 'NoneType' is not iterable\n",
      "Error processing user U63162: This model's maximum context length is 4097 tokens. However, your messages resulted in 7805 tokens. Please reduce the length of the messages.\n",
      "Error processing user U63162: argument of type 'NoneType' is not iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing user U11306: This model's maximum context length is 4097 tokens. However, your messages resulted in 13726 tokens. Please reduce the length of the messages.\n",
      "Error processing user U11306: argument of type 'NoneType' is not iterable\n",
      "Pecision: 0.23095238095238096\n",
      "Recall: 0.4642857142857143\n",
      "Weighted precision: 0.6611590592225737\n",
      "Weighted recall: 0.6822692318033932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "precision_all = []\n",
    "recall_all = []\n",
    "weighted_precision_all = []\n",
    "weighted_recall_all = []\n",
    "\n",
    "for i in users_list:\n",
    "    try:\n",
    "        # Example usage\n",
    "        user_id = i  # Specify the user ID for which you want to generate recommendations\n",
    "        df = create_evaluation_df(user_id)\n",
    "        \n",
    "        # The rest of your code that uses df goes here\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing user {i}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    df = create_evaluation_df(user_id)\n",
    "    # Calculate precision\n",
    "    precision = precision_score(df['True'], df['Predicted'])\n",
    "    precision_all.append(precision)\n",
    "\n",
    "    # Calculate recall\n",
    "    recall = recall_score(df['True'], df['Predicted'])\n",
    "    recall_all.append(recall)\n",
    "\n",
    "    # Calculate weighted precision\n",
    "    weighted_precision = precision_score(df['True'], df['Predicted'], average='weighted')\n",
    "    weighted_precision_all.append(weighted_precision)\n",
    "    \n",
    "    # Calculate weighted recall\n",
    "    weighted_recall = recall_score(df['True'], df['Predicted'], average='weighted')\n",
    "    weighted_recall_all.append(weighted_recall)\n",
    "\n",
    "precision_mean = np.mean(precision_all)\n",
    "recall_mean = np.mean(recall_all)\n",
    "weighted_precision_mean = np.mean(weighted_precision_all)\n",
    "weighted_recall_mean = np.mean(weighted_recall_all )\n",
    "\n",
    "\n",
    "print ((f\"Pecision: {precision_mean}\"))\n",
    "print ((f\"Recall: {recall_mean}\"))\n",
    "print ((f\"Weighted precision: {weighted_precision_mean}\"))\n",
    "print ((f\"Weighted recall: {weighted_recall_mean}\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
